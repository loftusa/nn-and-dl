{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228da6c0-cfb1-4172-bceb-046c6736b9b6",
   "metadata": {},
   "source": [
    "# Logistic Regression as a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb39046a-a7c7-4f4a-8a6f-87b93111d190",
   "metadata": {},
   "source": [
    "for week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde5c4c-43ef-449b-9436-8bf2da50c6e5",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7670a09d-430b-4d19-82fa-ab84eb49db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X = X.T  # because for some reason this video has the columns as observations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "948e252a-39e4-41ed-9f71-5742a3169d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKuklEQVR4nO3d76vW9R3H8ddrp0JbTW3KMJUdb4QQg2UcDogjnNGwJTViNxSKzEF3VhQbRA26sX8g2o0RhNWKXLFZQUSrRRlb6DI1t6XHhpMzPFJTkcqEJtZ7N85XsDjtfK/r+v663ns+QDrXdS7O531Rz77X9T2X348jQgDy+FrbAwCoFlEDyRA1kAxRA8kQNZDMBXX80IULF8bo6GgdP7pVp0+fbnS9ycnJxtaaP39+Y2tdfvnlja1lu7G1mjQ5OakTJ07M+ORqiXp0dFS7d++u40e3aufOnY2ut3nz5sbWuvnmmxtb64EHHmhsrTlz5jS2VpPGxsa+8nu8/AaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkikVte11tt+zfcj2fXUPBaB/s0Zte0TSryVdL+lKSRttX1n3YAD6U+ZIPS7pUEQcjogzkp6RdFO9YwHoV5mol0g6ct7tqeK+L7B9h+3dtncfP368qvkA9KiyE2UR8UhEjEXE2KJFi6r6sQB6VCbqo5KWnXd7aXEfgA4qE/Xbkq6wvdz2RZI2SHqh3rEA9GvWiyRExFnbd0p6RdKIpMciYn/tkwHoS6krn0TES5JeqnkWABXgE2VAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMrXs0JFVkztmSNLBgwcbW+vkyZONrTV37tzG1tqxY0dja0nSqlWrGl1vJhypgWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIpswOHY/ZPmb73SYGAjCYMkfq30haV/McACoya9QR8SdJzX3aH8BAKntPzbY7QDew7Q6QDGe/gWSIGkimzK+0npa0U9IK21O2f1L/WAD6VWYvrY1NDAKgGrz8BpIhaiAZogaSIWogGaIGkiFqIBmiBpIZ+m13jhw50thaTW6DIzW7Fc6CBQsaW6vJ58W2OwCGHlEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8mUuUbZMtvbbR+wvd/23U0MBqA/ZT77fVbSzyNir+1LJe2x/WpEHKh5NgB9KLPtzvsRsbf4+pSkCUlL6h4MQH96ek9te1TSSklvzfA9tt0BOqB01LYvkfSspHsi4uMvf59td4BuKBW17Qs1HfTWiHiu3pEADKLM2W9LelTSREQ8WP9IAAZR5ki9WtKtktba3lf8+WHNcwHoU5ltd96U5AZmAVABPlEGJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJDv5fWqVOnGltrzZo1ja0lNbu/VZPGx8fbHiE1jtRAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJlLjw4x/Yu238ttt35ZRODAehPmY+J/kfS2oj4pLhU8Ju2/xARf6l5NgB9KHPhwZD0SXHzwuJP1DkUgP6VvZj/iO19ko5JejUi2HYH6KhSUUfEZxFxlaSlksZtf2eGx7DtDtABPZ39jogPJW2XtK6WaQAMrMzZ70W25xdfz5V0naSDNc8FoE9lzn4vlvSE7RFN/0/gdxHxYr1jAehXmbPff9P0ntQAhgCfKAOSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogmaHfduejjz5qbK3169c3tlZmJ0+ebGytyy67rLG1uoIjNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyZSOurig/zu2uegg0GG9HKnvljRR1yAAqlF2252lkm6QtKXecQAMquyR+iFJ90r6/KsewF5aQDeU2aFjvaRjEbHnfz2OvbSAbihzpF4t6Ubbk5KekbTW9lO1TgWgb7NGHRH3R8TSiBiVtEHS6xFxS+2TAegLv6cGkunpckYR8YakN2qZBEAlOFIDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyQz9tjvz5s1rbK1du3Y1tlbTPv3008bW2rFjR2Nrbdq0qbG1uoIjNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyZT6mGhxJdFTkj6TdDYixuocCkD/evns9/cj4kRtkwCoBC+/gWTKRh2S/mh7j+07ZnoA2+4A3VA26u9FxNWSrpf0U9vXfPkBbLsDdEOpqCPiaPHPY5KelzRe51AA+ldmg7yv27703NeSfiDp3boHA9CfMme/vyXpedvnHv/biHi51qkA9G3WqCPisKTvNjALgArwKy0gGaIGkiFqIBmiBpIhaiAZogaSIWogmaHfdmfx4sWNrfXaa681tpYk7dy5s7G1nnzyycbWatJtt93W9giN40gNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAypaK2Pd/2NtsHbU/YXlX3YAD6U/az37+S9HJE/Nj2RZIurnEmAAOYNWrb8yRdI2mTJEXEGUln6h0LQL/KvPxeLum4pMdtv2N7S3H97y9g2x2gG8pEfYGkqyU9HBErJZ2WdN+XH8S2O0A3lIl6StJURLxV3N6m6cgBdNCsUUfEB5KO2F5R3HWtpAO1TgWgb2XPft8laWtx5vuwpNvrGwnAIEpFHRH7JI3VOwqAKvCJMiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSGfq9tBYsWNDYWk3vN7V58+bG1lqzZk1ja23fvr2xtf4fcaQGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpKZNWrbK2zvO+/Px7bvaWA2AH2Y9WOiEfGepKskyfaIpKOSnq93LAD96vXl97WS/hkR/6pjGACD6zXqDZKenukbbLsDdEPpqItrft8o6fczfZ9td4Bu6OVIfb2kvRHx77qGATC4XqLeqK946Q2gO0pFXWxde52k5+odB8Cgym67c1rSN2ueBUAF+EQZkAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8k4Iqr/ofZxSb3+9cyFkk5UPkw3ZH1uPK/2fDsiZvybU7VE3Q/buyNirO056pD1ufG8uomX30AyRA0k06WoH2l7gBplfW48rw7qzHtqANXo0pEaQAWIGkimE1HbXmf7PduHbN/X9jxVsL3M9nbbB2zvt3132zNVyfaI7Xdsv9j2LFWyPd/2NtsHbU/YXtX2TL1q/T11sUHAPzR9uaQpSW9L2hgRB1odbEC2F0taHBF7bV8qaY+kHw378zrH9s8kjUn6RkSsb3ueqth+QtKfI2JLcQXdiyPiw5bH6kkXjtTjkg5FxOGIOCPpGUk3tTzTwCLi/YjYW3x9StKEpCXtTlUN20sl3SBpS9uzVMn2PEnXSHpUkiLizLAFLXUj6iWSjpx3e0pJ/uM/x/aopJWS3mp5lKo8JOleSZ+3PEfVlks6Lunx4q3FluKim0OlC1GnZvsSSc9KuiciPm57nkHZXi/pWETsaXuWGlwg6WpJD0fESkmnJQ3dOZ4uRH1U0rLzbi8t7ht6ti/UdNBbIyLL5ZVXS7rR9qSm3yqttf1UuyNVZkrSVESce0W1TdORD5UuRP22pCtsLy9OTGyQ9ELLMw3MtjX93mwiIh5se56qRMT9EbE0IkY1/e/q9Yi4peWxKhERH0g6YntFcde1kobuxGap637XKSLO2r5T0iuSRiQ9FhH7Wx6rCqsl3Srp77b3Fff9IiJeam8klHCXpK3FAeawpNtbnqdnrf9KC0C1uvDyG0CFiBpIhqiBZIgaSIaogWSIGkiGqIFk/gvBgrDdv4x46gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[:, 0].reshape(8,8), cmap=\"Greys\")\n",
    "print(\"label: \", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96d32eae-2661-4e83-837f-7b84032f794a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1797)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape  # this feels weird"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf51b1-7b9e-4f76-8ee8-5c3ea1cb9cb4",
   "metadata": {},
   "source": [
    "### Setup\n",
    "we want $\\hat{y} = P(y = 1 | X)$, with $x \\in R^{n_x}, w \\in R^{n_x}, b \\in R$.  \n",
    "\n",
    "in logistic regression, $\\hat{y} = \\sigma (w^\\top x + b)$, with $\\sigma$ being the sigmoid function $\\frac{1}{1 + e^{-x}}$ which maps the real number line to {0, 1} with $lim_{x \\to -\\infty} = 0$, $\\lim_{x \\to 0} = \\frac{1}{2}$, and $\\lim_{x \\to \\infty} = 1$. Since $\\hat{y}$ is a probability, and we are trying to use our data to get clear predictions, we generally want to maximize $\\hat{y}$ to be as close as possible to 1 for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f16b5ea-ba1f-4ccf-80c7-175769bf4925",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loss function\n",
    "\n",
    "lots of ways to define, obvious one is $L(\\hat{y}, y) = 1/2 (\\hat{y} - y)^2$, but that's non-convex and so you find local minimums, so instead we use  \n",
    "\n",
    "$L(\\hat{y}, y) = -(y \\log \\hat{y} + (1 - y) \\log(1 - \\hat{y}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6405bc1-6e5d-452b-a83e-67af7c5e9595",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "$J(w, b) = \\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})$\n",
    "\n",
    "slight abuse of notation since $\\hat{y}^{(i)} = \\hat{y}^{(i)}(w, b)$, $\\hat{y}^{(i)} \\in R$ is the function $w^\\top x_i + b$ of $w$ and $b$ (also an abuse of notation, since $J(w, b)$ is actually $J(X, w, b)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dbab1c33-4056-44da-ab8d-bf79cfe07b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "16.0\n",
      "18.0\n",
      "1.0\n",
      "1.0\n",
      "---\n",
      "0.3250829733914482\n",
      "0.3250829733914482\n",
      "0.6931471805599453\n",
      "---\n",
      "2.0828626352604234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/j3gn7gkj60q66ts67tfg67hw0000gn/T/ipykernel_77223/977079358.py:6: UserWarning: Labels should be binary\n",
      "  warnings.warn(\"Labels should be binary\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def loss(yhat, y):\n",
    "    if y not in {0, 1}:\n",
    "        warnings.warn(\"Labels should be binary\")\n",
    "    return -1 * (y*np.log(yhat) + (1-y)*np.log(1-yhat))\n",
    "\n",
    "print(np.round(cost(.9999999, 1)))  # case where y = 1, cost(yhat, y) = -log(yhat)\n",
    "print(np.round(cost(.00000001, 0)))  # case where y = 0, cost(yhat, y) = -log(1-yhat)\n",
    "print(np.round(cost(.9999999, 0)))  # case where y = 0, cost(yhat, y) = -log(1-yhat)\n",
    "print(np.round(cost(.00000001, 1)))  # case where y = 0, cost(yhat, y) = -log(1-yhat)\n",
    "print(np.round(cost(.5, 0)))\n",
    "print(np.round(cost(.5, 1)))\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "print(loss(.1, .1))  # why aren't these 0?\n",
    "print(loss(.9, .9))  # why aren't these 0?\n",
    "print(loss(.5, .5))  # why aren't these 0?\n",
    "\n",
    "# not 0 because y = {0, 1}, can never be a non-integer, so the above inputs will never happen\n",
    "\n",
    "print(\"---\")\n",
    "print(cost(.1, .9))  # should be big"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867cf2a4-c680-4b54-8b11-d100e1403b45",
   "metadata": {},
   "source": [
    "## Gradient descent and viewing logistic regression as a NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893ed82-36ca-48b7-bfdd-0b6341f85f5a",
   "metadata": {},
   "source": [
    "by convention, $\\frac{dFinalOutputVar}{dvar}$ will be shorthand to $dvar$ in code, which is the derivative of the final output variable with respect to various intermediate quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472c98a-cc41-4b86-838f-dc2c78772be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def d_dw(w, alpha):\n",
    "    # denote as dw in code\n",
    "    \"\"\"\n",
    "    alpha: learning rate\n",
    "    \"\"\"\n",
    "    return w - alpha * np.nan  # TODO: np.nan = dJ(w)/dw\n",
    "\n",
    "def d_db(b, alpha):\n",
    "    # denote as db in code\n",
    "    return b - alpha * np.nan  # TODO: np.nan = dJ(b)/db\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
